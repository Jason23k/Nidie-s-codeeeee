# import sys
# import os
# sys.path.append(os.path.dirname(os.path.abspath(__file__)))  # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•

# from datasets.sliced_coco import SlicedCocoDetection
# from datasets.coco import make_coco_transforms

# # å‡è®¾ä½ çš„æ•°æ®è·¯å¾„å¦‚ä¸‹ï¼ˆè¯·æŒ‰å®é™…ä¿®æ”¹ï¼‰
# img_folder = "E:/ultralytics/NIR/Videos_frames"
# ann_file = "E:/ultralytics/Nidie-s-codeeeee/VODkkkk/VOD-master/annotations/train.json"

# # dataset = SlicedCocoDetection(
# #     img_folder=img_folder,
# #     ann_file=ann_file,
# #     transforms=make_coco_transforms("train"),
# #     slice_size=800,
# #     small_obj_threshold=900
# # )

# dataset = SlicedCocoDetection(
#     img_folder=img_folder,
#     ann_file=ann_file,
#     transforms=make_coco_transforms("train"),
#     slice_size=640,
#     small_obj_threshold=100,
#     return_original_prob=0.0,  # å¼ºåˆ¶åˆ‡ç‰‡
# )

# img, target = dataset[0]
# # print("å›¾åƒå°ºå¯¸:", img.size)
# # print("ç›®æ ‡æ•°é‡:", len(target["boxes"]))
# # print("Boxes ç¤ºä¾‹:", target["boxes"][:2])

# print("âœ… æˆåŠŸåŠ è½½åˆ‡ç‰‡æ ·æœ¬ï¼")
# print("å›¾åƒç±»å‹:", type(img))
# print("boxes å½¢çŠ¶:", target["boxes"].shape)
# print("labels:", target["labels"])

# test_sliced_datasets.py
# from datasets.sliced_coco import create_sliced_coco_dataset
# from torch.utils.data import DataLoader
# from datasets.coco import CocoDetection  # å‡è®¾ä½ æœ‰è¿™ä¸ªç±»ï¼Œç±»ä¼¼ torchvision.datasets.CocoDetection

# def test_sliced_dataset():
#     # é…ç½®è·¯å¾„ï¼ˆè¯·æ›¿æ¢æˆä½ çš„å®é™…è·¯å¾„ï¼‰
#     original_coco_json = "E:/ultralytics/NIR/Videos_frames"
#     original_image_dir = "E:/ultralytics/Nidie-s-codeeeee/VODkkkk/VOD-master/annotations/train.json"
#     sliced_output_dir = "./sliced_dataset"

#     # 1. ç”Ÿæˆåˆ‡ç‰‡æ•°æ®é›†
#     sliced_json, sliced_img_dir = create_sliced_coco_dataset(
#         coco_annotation_path=original_coco_json,
#         image_dir=original_image_dir,
#         output_dir=sliced_output_dir,
#         slice_height=640,
#         slice_width=640,
#         overlap_height_ratio=0.2,
#         overlap_width_ratio=0.2,
#         min_area_ratio=0.1,
#     )

#     # 2. ç”¨ä½ ç°æœ‰çš„ CocoDetection åŠ è½½åˆ‡ç‰‡åçš„æ•°æ®
#     dataset = CocoDetection(
#         img_folder=sliced_img_dir,
#         ann_file=sliced_json,
#         transforms=None,  # æš‚æ—¶ä¸åŠ  transform
#     )

#     # 3. ç®€å•æµ‹è¯•åŠ è½½
#     print(f"åˆ‡ç‰‡åæ•°æ®é›†å¤§å°: {len(dataset)}")
#     for i in range(min(3, len(dataset))):
#         img, target = dataset[i]
#         print(f"æ ·æœ¬ {i}: å›¾åƒå°ºå¯¸ {img.size}, ç›®æ ‡æ•° {len(target['boxes'])}")

# if __name__ == "__main__":
#     test_sliced_dataset()

# test_sliced_dataset.py
# test_sliced_dataset.py
from datasets.sliced_coco import create_sliced_coco_dataset
from datasets.coco import make_coco_transforms

def make_simple_transforms():
    return Compose([
        ToTensor(),
        Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])

# å°è¯•å¯¼å…¥ä½ çš„ CocoDetection
try:
    from datasets.coco import CocoDetection
except Exception:
    from torchvision.datasets import CocoDetection
    print("âš ï¸ ä½¿ç”¨ torchvision çš„ CocoDetection")

def test_sliced_dataset():
    # ğŸ”´ æ›¿æ¢ä¸ºä½ çš„å®é™…è·¯å¾„
    original_coco_json = "E:/ultralytics/Nidie-s-codeeeee/VODkkkk/VOD-master/annotations/train_clean.json"
    original_image_dir = "E:/ultralytics/NIR/Videos_frames"
    sliced_output_dir = "./sliced_dataset"

    # ç”Ÿæˆåˆ‡ç‰‡æ•°æ®é›†
    sliced_json, sliced_img_dir = create_sliced_coco_dataset(
        coco_annotation_path=original_coco_json,
        image_dir=original_image_dir,
        output_dir=sliced_output_dir,
        slice_height=640,
        slice_width=640,
        overlap_height_ratio=0.2,
        overlap_width_ratio=0.2,
        min_area_ratio=0.1,
    )

    # åŠ è½½ï¼ˆæ³¨æ„ï¼šå›¾åƒåœ¨ sliced_img_dir æ ¹ç›®å½•ï¼Œä¸æ˜¯å­æ–‡ä»¶å¤¹ï¼‰
    dataset = CocoDetection(
        img_folder=sliced_img_dir, 
        ann_file=sliced_json, 
        transforms=make_simple_transforms(),    # è°ƒå¼æˆåŠŸåæ”¹ä¸º make_coco_transforms("train") æˆ–è®¾ä¸º None æµ‹è¯•åŸå§‹ PIL å›¾åƒ
        return_masks=False
    )

    print(f"ğŸ“Œ åˆ‡ç‰‡åæ•°æ®é›†å¤§å°: {len(dataset)}")
    for i in range(min(3, len(dataset))):
        img, targets = dataset[i]
        num_boxes = len(targets) if isinstance(targets, list) else (
            len(targets["boxes"]) if "boxes" in targets else "N/A"
        )
        print(f"  æ ·æœ¬ {i}: å›¾åƒå°ºå¯¸ {img.size}, ç›®æ ‡æ•°: {num_boxes}")

if __name__ == "__main__":
    test_sliced_dataset()